{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3v9p59dC-5y"
   },
   "source": [
    "# Intro\n",
    "In the Name of Allah\n",
    "\n",
    "Sentiment analysis is a technique through which you can analyze a piece of text to determine the sentiment behind it. In this notebook, we're going to train a Naïve Bayes Classifier for the task of sentiment analysis on Imdb movie reviews dataset.\n",
    "\n",
    "**Please pay attention to these notes:**\n",
    "\n",
    "<br/>\n",
    "\n",
    "- **Assignment Due:** 1401/09/20 23:59\n",
    "- Write your code in the cells denoted by:\n",
    "```\n",
    "######## Your Code Here ########\n",
    "```\n",
    "- You can add more cells if necessary\n",
    "- Finding any sort of copying will zero down your grade.\n",
    "- When your solution is ready to submit, don't forget to set the name of this notebook like  \"Name_StudentID.ipynb\".\n",
    "- If you have any questions about this assignment, feel free to drop us a line. You can also ask your questions on the telegram group.\n",
    "- You must run this notebook on Google Colab platform.\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iryuq7HoLy3j"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W-R7b4iAuKnP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mehxi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mehxi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mehxi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mehxi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "Cj9uxuvswOpA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-10 18:02:18--  https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 66212309 (63M) [text/plain]\r\n",
      "Saving to: ‘IMDB-Dataset.csv.2’\r\n",
      "\r\n",
      "IMDB-Dataset.csv.2    9%[>                   ]   5.76M   589KB/s    eta 86s    ^C\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqW0otKAQWB9"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load dataset and make it to pandas dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v4ujxrR8QkeU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n5  Probably my all-time favorite movie, a story o...  positive\n6  I sure would like to see a resurrection of a u...  positive\n7  This show was an amazing, fresh & innovative i...  negative\n8  Encouraged by the positive comments about this...  negative\n9  If you like original gut wrenching laughter yo...  positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Probably my all-time favorite movie, a story o...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I sure would like to see a resurrection of a u...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>This show was an amazing, fresh &amp; innovative i...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Encouraged by the positive comments about this...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>If you like original gut wrenching laughter yo...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Your Code Here ########\n",
    "df =  pd.read_csv('IMDB-Dataset.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "370oVEkqe8mp"
   },
   "source": [
    "# Preprocess\n",
    "The first step of NLP is text preprocessing. Data cleaning is a very crucial step in any machine learning model, but more so for NLP. Without the cleaning process, the dataset is often a cluster of words that the computer doesn’t understand. Raw data over a properly or improperly formed sentence is not always desirable as it contains lot of unwanted components like null/html/links/url/emoji/stopwords etc. So in this step, this unwanted components are removed for better performance and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this part I make class Preprocess to do preprocess on one column of data-frame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HWID_NYBk1oN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review sentiment\n0  one reviewers mentioned watching oz episode yo...  positive\n1  wonderful little production filming technique ...  positive\n2  thought wonderful way spend time hot summer we...  positive\n3  basically theres family little boy jake thinks...  negative\n4  petter matteis love time money visually stunni...  positive\n5  probably alltime favorite movie story selfless...  positive\n6  sure would like see resurrection dated seahunt...  positive\n7  show amazing fresh innovative idea first aired...  negative\n8  encouraged positive comments film looking forw...  negative\n9  like original gut wrenching laughter like movi...  positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one reviewers mentioned watching oz episode yo...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wonderful little production filming technique ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thought wonderful way spend time hot summer we...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically theres family little boy jake thinks...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter matteis love time money visually stunni...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>probably alltime favorite movie story selfless...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>sure would like see resurrection dated seahunt...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>show amazing fresh innovative idea first aired...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>encouraged positive comments film looking forw...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>like original gut wrenching laughter like movi...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Your Code Here ########\n",
    "\n",
    "# remove html tage\n",
    "def remove_tags(string):\n",
    "    result = re.sub('<.*?>','',string)\n",
    "    return result\n",
    "\n",
    "\n",
    "# remove  links\n",
    "def remove_links(string):\n",
    "    result = re.sub('http[s]?:\\/\\/.*','',string)\n",
    "    return result\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "\n",
    "    def __init__(self, data_frame,column_name):\n",
    "        self.df = data_frame\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def make_lower_case(self):\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda x: x.lower())\n",
    "\n",
    "    def remove_tags(self):\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda cw : remove_tags(cw))\n",
    "\n",
    "    def remove_link(self):\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda cw : remove_links(cw))\n",
    "\n",
    "    def remove_number(self):\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "    def remove_punctuations(self):\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    def remove_double_spaces(self):\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "    def remove_emoji(self):\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "        # add new stopwords to stop words.\n",
    "        new_stopwords = ['<*>']\n",
    "        stop_words.extend(new_stopwords)\n",
    "        # remove stopwords not .\n",
    "        stop_words.remove('not')\n",
    "\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "    def do_lemmatization(self):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        self.df[self.column_name] = self.df[self.column_name].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "\n",
    "    def do_all_preprocess(self):\n",
    "\n",
    "        self.make_lower_case()\n",
    "        self.remove_tags()\n",
    "        self.remove_link()\n",
    "        self.remove_number()\n",
    "        self.remove_punctuations()\n",
    "        self.remove_punctuations()\n",
    "        self.remove_double_spaces()\n",
    "        self.remove_emoji()\n",
    "        self.remove_stopwords()\n",
    "        self.do_lemmatization()\n",
    "        return self.df\n",
    "\n",
    "model_pre = Preprocess(df,\"review\")\n",
    "new_df = model_pre.do_all_preprocess()\n",
    "new_df.to_csv(\"out.csv\")\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeB0RNlJ89_g"
   },
   "source": [
    "<font size=\"5\">Split the dataset</font>\n",
    "\n",
    "Data splitting, or commonly known as train-test split, is the partitioning of data into subsets for model training and evaluation separately. Since the test set is not specified beforehand, we have to split the dataset into train and test set in an ideal proportion. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "hZT57Dblyf8Y"
   },
   "outputs": [],
   "source": [
    "######## Your Code Here ########\n",
    "X_train, X_test, y_train, y_test = tts(df['review'], df['sentiment'],random_state=12, train_size = .80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "data": {
      "text/plain": "35235    disagree people saying lousy horror film good ...\n36936    husbandandwife doctor team carole niles nelson...\n46486    like cast pretty much however story sort unfol...\n27160    movie awful bad cant bear expend anything word...\n19490    purchased blood castle dvd ebay bucks not know...\n                               ...                        \n36482    strange thing see film scenes work rather weak...\n40177    saw cheap dvd release title entity force since...\n19709    one peculiar oftused romance movie plots one s...\n38555    nothing positive say meandering nonsense huffi...\n14155    low moments life bewildered depressed sitting ...\nName: review, Length: 40000, dtype: object"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "data": {
      "text/plain": "34622    hard tell noonan marshall trying ape abbott co...\n1163     well startas one reviewers said know youre rea...\n7637     wife kids opinion absolute abc classic havent ...\n7045     surprise basic copycat comedy classic nutty pr...\n43847    josef von sternberg directs magnificent silent...\n                               ...                        \n29299    yes fast times wannabe still decent entertainm...\n29224    run dont walk rent movie rereleased excellent ...\n16503    docudrama would expect richard attenborough ma...\n40559    nepotism capitol world comes another junk flic...\n24396    moviemakers even preview released script jumps...\nName: review, Length: 10000, dtype: object"
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnesufkigiPw"
   },
   "source": [
    "# Training\n",
    "Use Naive Beyes algorithm to train a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this part I make class MehNaiveByes with algorithm of naive byes in fit and predict. with base of vector given Item."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "yf2dHGlKurSf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 40000, n_features: 185662\n"
     ]
    }
   ],
   "source": [
    "######## Your Code Here ########\n",
    "class MehNaiveBeyes:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.p_pos = 0\n",
    "        self.p_neg = 0\n",
    "        self.list_of_p_pos = []\n",
    "        self.list_of_p_neg = []\n",
    "\n",
    "    def fit(self,vectors_feature,list_label,name_pos,name_neg):\n",
    "\n",
    "        # At first, I find index of label pos and neg\n",
    "        numbers_of_item_pos_label = [i for i in range(len(list_label)) if list_label[i] == name_pos]\n",
    "        numbers_of_item_neg_label = [i for i in range(len(list_label)) if list_label[i] == name_neg]\n",
    "\n",
    "        # Then I sum all vector of passive\n",
    "        list_numbers_pos_item = np.zeros(vectors_feature.shape[1])\n",
    "        for index in numbers_of_item_pos_label:\n",
    "            item = vectors_feature[index].toarray()[0]\n",
    "\n",
    "            list_numbers_pos_item = list_numbers_pos_item + item\n",
    "\n",
    "\n",
    "        # And all vector of negative\n",
    "        list_numbers_neg_item = np.zeros(vectors_feature.shape[1])\n",
    "        for index in numbers_of_item_neg_label:\n",
    "            item = vectors_feature[index].toarray()[0]\n",
    "            list_numbers_neg_item = list_numbers_neg_item + item\n",
    "\n",
    "        # Then Find Percentage of pas class and neg class\n",
    "        self.p_pos = len(list_numbers_pos_item)/(len(list_numbers_pos_item)+len(list_numbers_neg_item))\n",
    "        self.p_neg = len(list_numbers_neg_item)/(len(list_numbers_pos_item)+len(list_numbers_neg_item))\n",
    "\n",
    "\n",
    "        # then for all element I find P(element | neg) P(element | pas)\n",
    "        sum_list_numbers_pos_item = sum(list_numbers_pos_item)\n",
    "        self.list_of_p_pos = np.array([(item+1)/(sum_list_numbers_pos_item+vectors_feature.shape[1]) for item in list_numbers_pos_item])\n",
    "\n",
    "        sum_list_numbers_neg_item = sum(list_numbers_neg_item)\n",
    "        self.list_of_p_neg = np.array([(item+1)/(sum_list_numbers_neg_item+vectors_feature.shape[1]) for item in list_numbers_neg_item])\n",
    "\n",
    "\n",
    "    def predict(self,vectors_feature,name_pos,name_neg):\n",
    "\n",
    "\n",
    "        label_predict = []\n",
    "        # We have percentage of pas and neg and P(element | neg) P(element | pas) then we can find  Naive Beyes for each word\n",
    "        # I use np.log because my score became vary small if use original formula\n",
    "        for item in vectors_feature:\n",
    "            item_arr = item.toarray()[0]\n",
    "\n",
    "            pos_res = np.log(self.p_pos)\n",
    "            neg_res = np.log(self.p_neg)\n",
    "\n",
    "            pos_res = pos_res + np.log(self.list_of_p_pos.dot(item_arr))\n",
    "            neg_res = neg_res + np.log(self.list_of_p_neg.dot(item_arr))\n",
    "\n",
    "            # Each part have more probability I label with it .\n",
    "            if pos_res >= neg_res:\n",
    "                 label_predict.append(name_pos)\n",
    "            else :\n",
    "                 label_predict.append(name_neg)\n",
    "\n",
    "        return label_predict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word' , stop_words='english',)\n",
    "tfidfvectorizer.fit(X_train)\n",
    "tfidf_train = tfidfvectorizer.transform(X_train)\n",
    "print(\"n_samples: %d, n_features: %d\" % tfidf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "naive_beyes = MehNaiveBeyes()\n",
    "naive_beyes.fit(tfidf_train, np.array(y_train),\"positive\",\"negative\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "tfidf_test = tfidfvectorizer.transform(X_test)\n",
    "y_pred = naive_beyes.predict(tfidf_test,\"positive\",\"negative\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvSqU886lMDN"
   },
   "source": [
    "# Test\n",
    "Now you need to run inference on your test set"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Testing, I write class that have all function I need like accuracy_score, precision, recall, f1_measure and confusion_matrix_2D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "class Me_Calcu_Accuracy:\n",
    "    @staticmethod\n",
    "    def accuracy_score(y_main, y_pred):\n",
    "        y_main = list(y_main)\n",
    "        y_pred = list(y_pred)\n",
    "        # find all true Item model find\n",
    "        upper = sum([1 for i in range(len(y_main)) if y_main[i]==y_pred[i]])\n",
    "        # find accuracy\n",
    "        return upper/len(y_main)\n",
    "    @staticmethod\n",
    "    def precision(y_main, y_pred,name):\n",
    "        y_main = list(y_main)\n",
    "        y_pred = list(y_pred)\n",
    "        # find true possetive\n",
    "        true_pos = sum([1 for i in range(len(y_main)) if y_main[i]==y_pred[i] and y_main[i] == name])\n",
    "        # Find all item was pos in our prediction list\n",
    "        down = sum([1 for i in range(len(y_main)) if  y_pred[i] == name])\n",
    "        return true_pos/down\n",
    "\n",
    "    @staticmethod\n",
    "    def recall(y_main, y_pred,name):\n",
    "        y_main = list(y_main)\n",
    "        y_pred = list(y_pred)\n",
    "\n",
    "        # find true possetive\n",
    "        true_pos = sum([1 for i in range(len(y_main)) if y_main[i]==y_pred[i] and y_main[i] == name])\n",
    "        # Find all item was pos in our main list\n",
    "        pos =  sum([1 for i in range(len(y_main)) if  y_main[i] == name])\n",
    "\n",
    "        return true_pos/pos\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_measure(y_main, y_pred,name):\n",
    "        precision = Me_Calcu_Accuracy.precision(y_main, y_pred,name)\n",
    "        recall = Me_Calcu_Accuracy.recall(y_main, y_pred,name)\n",
    "        return 2 * (precision * recall)/(precision+recall)\n",
    "\n",
    "    @staticmethod\n",
    "    def confustion_matrix_2D(y_main, y_pred):\n",
    "        y_main = list(y_main)\n",
    "        y_pred = list(y_pred)\n",
    "        set_item = set(y_main)\n",
    "        list_set = list(set_item)\n",
    "        set_item = (list_set[1],list_set[0])\n",
    "        if len(set_item) != 2:\n",
    "            raise \"this item for binary\"\n",
    "\n",
    "        confusion_matrix = []\n",
    "        k = 0\n",
    "        for item in set_item :\n",
    "            list_make = []\n",
    "            if k == 0:\n",
    "                list_make.append(sum([1 for i in range(len(y_main)) if y_main[i]==y_pred[i] and y_main[i] == item]))\n",
    "                list_make.append(sum([1 for i in range(len(y_main)) if y_main[i]!=y_pred[i] and y_pred[i] == item]))\n",
    "                k +=1\n",
    "            else:\n",
    "                list_make.append(sum([1 for i in range(len(y_main)) if y_main[i]!=y_pred[i] and y_pred[i] == item]))\n",
    "                list_make.append(sum([1 for i in range(len(y_main)) if y_main[i]==y_pred[i] and y_main[i] == item]))\n",
    "\n",
    "\n",
    "            confusion_matrix.append(list_make)\n",
    "\n",
    "        print(set_item[0],set_item[1])\n",
    "        for item in confusion_matrix:\n",
    "            print(item)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "0Pr2rqDwlPpU"
   },
   "outputs": [],
   "source": [
    "######## Your Code Here ########\n",
    "y_pred = naive_beyes.predict(tfidf_test,\"positive\",\"negative\")\n",
    "score1 = Me_Calcu_Accuracy.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7778"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBS9HFA8iWlX"
   },
   "source": [
    "# Evaluation\n",
    "After training is finished, we need some metrics to evaluate the trained model on the test set. Here, you need to write code for utilizing the metrics bellow without the sklearn libraries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIeLqza_llil"
   },
   "source": [
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "tjzeJiZWloV8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  0.8925193465176269\n"
     ]
    }
   ],
   "source": [
    "######## Your Code Here ########\n",
    "precision = Me_Calcu_Accuracy.precision(y_test,y_pred,\"positive\")\n",
    "print(\"precision : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go2HcCbTln_M"
   },
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "phSzQZ6JlsV1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall :  0.6249247441300422\n"
     ]
    }
   ],
   "source": [
    "######## Your Code Here ########\n",
    "recall = Me_Calcu_Accuracy.recall(y_test,y_pred,\"positive\")\n",
    "print(\"recall : \",recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUM6OQNAlqks"
   },
   "source": [
    "F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "EnzV0G1tlsA8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_measure L  0.735127478753541\n"
     ]
    }
   ],
   "source": [
    "######## Your Code Here ########\n",
    "f_measure = Me_Calcu_Accuracy.f1_measure(y_test,y_pred,\"positive\")\n",
    "print(\"f1_measure L \",f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmo7-ZsEytmo"
   },
   "source": [
    "Confustion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "sVFCeotJy1DZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive negative\n",
      "[3114, 375]\n",
      "[1869, 4642]\n"
     ]
    }
   ],
   "source": [
    "######## Your Code Here ########\n",
    "Me_Calcu_Accuracy.confustion_matrix_2D(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bayes_Classifier_Notebook_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
