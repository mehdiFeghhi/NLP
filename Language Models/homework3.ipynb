{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZSmCY-m890C"
   },
   "source": [
    "## Name & ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "0fc27Edy8juu"
   },
   "outputs": [],
   "source": [
    "student_name = \"Mehdi Feghhi\"\n",
    "student_id = 401722136"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hJjJIvi9D9m"
   },
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Kj_IHw3X9QkT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iubaGuk29UNz"
   },
   "source": [
    "# Implementation section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GoZDydj-aow"
   },
   "source": [
    "## part a"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "AT First, I replace punctuation with space punctuation space .\n",
    "Then I split word by space AND half space to token the sentences.\n",
    "And I return tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "xoPx75Qn9TLW"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    for punc in string.punctuation:\n",
    "        text = text.replace(punc,' '+punc+' ')\n",
    "\n",
    "    text_without_space = re.split(\" |\\u200c\",text)\n",
    "\n",
    "    return text_without_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgs6c2PE-dRi"
   },
   "source": [
    "## part b"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "At first, I add one <END> at end of sentence tokens then I add (n-1) <start> add first of tokens n is number of grams.\n",
    "Then from first taken after <start> I found all n-1 tokens before it and put it on list then I make it to tuple like return of sample.\n",
    "this list include n-1 tuple tokens before this token and token."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "eHZqhCQ3-hPN"
   },
   "outputs": [],
   "source": [
    "def ngrams(n, tokens):\n",
    "    tokens.append('<END>')\n",
    "    for i in range(n-1):\n",
    "        tokens = ['<START>']+ tokens\n",
    "    # print(tokens)\n",
    "    list_ngram = []\n",
    "    for i in range(n-1,len(tokens)):\n",
    "        # print(i)\n",
    "        item_before = []\n",
    "        for j in range(n-1):\n",
    "            # print(j)\n",
    "            item_before = [tokens[i-j-1]]+ item_before\n",
    "        list_ngram.append((tuple(item_before),tokens[i]))\n",
    "\n",
    "    return list_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQd1vhOR-gXD"
   },
   "source": [
    "## NgramModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "-S0Mkxzx-sYc"
   },
   "outputs": [],
   "source": [
    "class NgramModel(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.res_str =[]\n",
    "\n",
    "    '''\n",
    "        res_str include ngrams of each sentence as input we use ngrams function in this section\n",
    "    '''\n",
    "    def update(self, sentence):\n",
    "        self.res_str += ngrams(self.n,tokenize(sentence))\n",
    "\n",
    "    '''\n",
    "        for calculate prob of each token with its context .\n",
    "        at first must calculate number of context in the all res_str as denominator.\n",
    "        then calculate number of tokens come after this context as numerator.\n",
    "        at end we return division of numerator and denominator for probability .\n",
    "    '''\n",
    "\n",
    "    def prob(self, context, token):\n",
    "\n",
    "        denominator = 0\n",
    "        numerator = 0\n",
    "        # print(self.res_str)\n",
    "        for item in self.res_str:\n",
    "            if item[0] == context:\n",
    "                denominator += 1\n",
    "                if item[1] == token:\n",
    "                    numerator += 1\n",
    "\n",
    "        return numerator/denominator\n",
    "\n",
    "    '''\n",
    "        Find all token that come with context\n",
    "        I sort it by alphabet .\n",
    "        then I find random.number\n",
    "        then I find word that sum of the prob of word before it with this token upper than r .\n",
    "        then I find prob of each item .\n",
    "\n",
    "    '''\n",
    "\n",
    "    def random_token(self, context):\n",
    "\n",
    "        r = random.random()\n",
    "        map_to_probs = {}\n",
    "        token_of_interest = [item[1] for item in self.res_str if item[0]==context]\n",
    "        for token in token_of_interest:\n",
    "            map_to_probs[token] = self.prob(context, token)\n",
    "\n",
    "        summ = 0\n",
    "        for token in sorted(map_to_probs):\n",
    "            summ += map_to_probs[token]\n",
    "            if summ > r:\n",
    "                return token\n",
    "        return \"<END>\"\n",
    "\n",
    "    \"\"\"\n",
    "    I generate a text with number token function give with random_token.\n",
    "    for better result I make context with use each word I generate with random_token.\n",
    "    \"\"\"\n",
    "    def random_text(self, token_count):\n",
    "\n",
    "        list_item = []\n",
    "        \"add n-1 <start> for context for first context\"\n",
    "        new_context = [\"<START>\" for i in range(self.n-1)]\n",
    "\n",
    "        # new_context = tuple(new_context)\n",
    "        for count in range(token_count):\n",
    "\n",
    "            find_item = self.random_token(tuple(new_context))\n",
    "            list_item.append(find_item)\n",
    "            if self.n > 1 :\n",
    "                if find_item == '<END>':\n",
    "                    \"if find <end> as item I want to make new sentence because of that change the new context like first one .\"\n",
    "                    new_context = [\"<START>\" for i in range(self.n-1)]\n",
    "                else:\n",
    "                    \"update new context with new word we find \"\n",
    "                    new_context.pop(0)\n",
    "                    new_context.append(find_item)\n",
    "\n",
    "        return \" \".join(list_item)\n",
    "\n",
    "    \"\"\"\n",
    "    calculate perplexity like formulation . at first I find ngram list then I calculate prob of each token and then mul it with res mul of befor token in sentence.\n",
    "    \"\"\"\n",
    "    def perplexity(self, sentence):\n",
    "        ngram_list = ngrams(self.n,tokenize(sentence))\n",
    "        res_mul = 1\n",
    "        for item in ngram_list:\n",
    "            res_mul = res_mul * self.prob(item[0],item[1])\n",
    "\n",
    "        return (1/res_mul)**(1/len(ngram_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAo04nZ1-sl3"
   },
   "source": [
    "## part f"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "With this function I read all sentence of text file and update of ngram model that make with n."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "LyaVj895-s3q"
   },
   "outputs": [],
   "source": [
    "def create_ngram_model(n, path):\n",
    "    model = NgramModel(n)\n",
    "    file1 = open(path, 'r')\n",
    "    lines = file1.readlines()\n",
    "\n",
    "    # x = 0\n",
    "    # list_add = []\n",
    "    for line in lines:\n",
    "         model.update(line.strip())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xhw4AiX-RIi"
   },
   "source": [
    "# Check section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II8iwx9K_Rii"
   },
   "source": [
    "## part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "['منم', 'کاوه', 'دادخواه', 'یکی', 'بی', 'زیان', 'مرد', 'آهنگرم']"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"منم کاوه دادخواه یکی بی‌زیان مرد آهنگرم\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "SPtM8HJWKJHB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['I', \"'\", 'm', 'Kaveh', ',', '', 'and', 'a', 'blacksmith', ',', '', 'sire']"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"I'm Kaveh, and a blacksmith, sire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrfGg1I2_VsP"
   },
   "source": [
    "## part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "022Yi5qDVE7I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[((), 'کاوه'), ((), 'اهنگر'), ((), 'دادخواه'), ((), '<END>')]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(1, [\"کاوه\", \"اهنگر\", \"دادخواه\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "xbY1-2GcVVOF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(('<START>',), 'کاوه'),\n (('کاوه',), 'اهنگر'),\n (('اهنگر',), 'دادخواه'),\n (('دادخواه',), '<END>')]"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(2, [\"کاوه\", \"اهنگر\", \"دادخواه\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "C2cn8QdOVuOA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(('<START>', '<START>'), 'کاوه'),\n (('<START>', 'کاوه'), 'اهنگر'),\n (('کاوه', 'اهنگر'), 'دادخواه'),\n (('اهنگر', 'دادخواه'), '<END>')]"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(3, [\"کاوه\", \"اهنگر\", \"دادخواه\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_mIgLXb_ZCg"
   },
   "source": [
    "## part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "yrpwYkUNVzqZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.1\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "m = NgramModel(1)\n",
    "m.update(\"کاوه آهنگر ضحاک ستمگر\")\n",
    "m.update(\"کاوه آهنگر کاوه آهنگر\")\n",
    "print(m.prob((), \"کاوه\"))\n",
    "print(m.prob((), \"ضحاک\"))\n",
    "print(m.prob((), \"<END>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "KOJ4d3oDWDG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.3333333333333333\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "m = NgramModel(2)\n",
    "m.update(\"کاوه آهنگر ضحاک ستمگر\")\n",
    "m.update(\"کاوه آهنگر کاوه آهنگر\")\n",
    "print(m.prob((\"<START>\",), \"کاوه\"))\n",
    "print(m.prob((\"آهنگر\",), \"ضحاک\"))\n",
    "print(m.prob((\"کاوه\",), \"ماردوش\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m4p5AQw_a5K"
   },
   "source": [
    "## part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Hi-h7NwcXdf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['<END>', 'کاوه', 'کاوه', 'آهنگر', 'آهنگر', 'آهنگر', 'ضحاک', 'کاوه']"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModel(1)\n",
    "m.update(\"کاوه آهنگر ضحاک ستمگر\")\n",
    "m.update(\"کاوه آهنگر کاوه آهنگر\")\n",
    "random.seed(1)\n",
    "[m.random_token(()) for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "qeq9hA3DYCDr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['کاوه', 'کاوه', 'کاوه', 'کاوه', 'کاوه', 'کاوه']\n",
      "['کاوه', '<END>', 'ضحاک', 'ضحاک', 'ضحاک', '<END>']\n"
     ]
    }
   ],
   "source": [
    "m = NgramModel(2)\n",
    "m.update(\"کاوه آهنگر ضحاک ستمگر\")\n",
    "m.update(\"کاوه آهنگر کاوه آهنگر\")\n",
    "random.seed(2)\n",
    "print([m.random_token((\"<START>\",)) for i in range(6)])\n",
    "print([m.random_token((\"آهنگر\",)) for i in range(6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKu0rMZG_d2q"
   },
   "source": [
    "## part e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "fmvyqyyEY06R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'<END> کاوه کاوه آهنگر آهنگر آهنگر ضحاک کاوه <END> <END> کاوه آهنگر کاوه <END> آهنگر کاوه'"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModel(1)\n",
    "m.update(\"کاوه آهنگر ضحاک ستمگر\")\n",
    "m.update(\"کاوه آهنگر کاوه آهنگر\")\n",
    "random.seed(1)\n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "csZXsGzUZTdR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'کاوه آهنگر <END> کاوه آهنگر کاوه آهنگر <END> کاوه آهنگر ضحاک ستمگر <END> کاوه آهنگر کاوه'"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModel(2)\n",
    "m.update(\"کاوه آهنگر ضحاک ستمگر\")\n",
    "m.update(\"کاوه آهنگر کاوه آهنگر\")\n",
    "random.seed(2)\n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrDUyk5p_h1E"
   },
   "source": [
    "## part f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "cgaHPLJQJ-Mp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'<END> to blow nobleman and ” seal he , aside  him this . drew city'"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(1, \"Shahnameh.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "ZwdYGr4PJ-QL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'یکی زیان <END> پیکر نهادند فرخ چو مردم نبایدت آمدش گوا گذشتن <END> ماند شاهی برآمد'"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(1, \"Zahhak.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "QiCUMmAlZdgN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'خاکستری بی کشد ته و است وز پرده برف درون کرد شکاف بی <END> آهسته سیاه'"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(1, \"Akhavan.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "d1PsU5r8J78v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Then on us now conquered my father had looked on their chests ,  branches of'"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(2, \"Shahnameh.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "HfwsWjcbJ8AB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'که بر و زد دست بردند گرز پیکر بدیشان نمود <END>  <END> که ضحاک بیرون'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(2, \"Zahhak.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "ypjgHiUscV7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'چه عمر راحتی دنیای خوبی <END> نه حتی جنگلی کوچک ، خون گرگان  . '"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(2, \"Akhavan.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "xpI_z6zBJ8fs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' <END>  <END> this you will attain to your love he <END> brought a large'"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(3, \"Shahnameh.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "Hk6YE45zJ8jM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'ز ایوان برون شد ز درگاه شاه <END> ز ایوان برون شد خروشان به کوی <END>'"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(3, \"Zahhak.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "80q30x52ejVp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' <END> بلای نیستی ، سرمای پر سوز <END>  <END> ولی شلاق  ! '"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(3, \"Akhavan.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "n74i8f7HJ_xZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'whose rule everything exists from the least straw to elephants and lions a fool , '"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(4, \"Shahnameh.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "n-x8xdDhJ_1H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'بباید بدین بود همداستان <END> کسی کاو هوای فریدون کند <END>  <END> ببخشیدشان جامه و'"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(4, \"Zahhak.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "q66jTUwhewfP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'ولیکن عزت آزادگی را <END> دو دشمن در کمین ماست ، دایم <END> نگهبانیم ، آزادیم'"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No random seeds, so your results may vary\n",
    "m = create_ngram_model(4, \"Akhavan.txt\") \n",
    "m.random_text(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYpoN-xc_ngf"
   },
   "source": [
    "## part g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "nYiioKZgfavy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3.815714141844439"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModel(1)\n",
    "m.update(\"کاوه آهنگر ضحاک ستمگر\")\n",
    "m.update(\"کاوه آهنگر کاوه آهنگر\")\n",
    "m.perplexity(\"کاوه آهنگر\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "i2fQqAaQfxYb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1.4422495703074083"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NgramModel(2)\n",
    "m.update(\"کاوه آهنگر ضحاک ستمگر\")\n",
    "m.update(\"کاوه آهنگر کاوه آهنگر\")\n",
    "m.perplexity(\"کاوه آهنگر\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
